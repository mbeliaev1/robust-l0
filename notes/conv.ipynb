{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import linalg\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "root = os.path.abspath('/home/mbeliaev/home/code/robust-l0/')\n",
    "sys.path.append(root)\n",
    "device = 'cuda:2'\n",
    "\n",
    "from utils.helpers import toeplitz_1_ch\n",
    "from utils.trunc import trunc_conv, trunc_conv_new\n",
    "from utils.models import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing independent clip\n",
    "bs = 256\n",
    "in_ch = 1\n",
    "out_ch = 1\n",
    "l = 28\n",
    "ker_dim = 3\n",
    "out_dim = l - ker_dim + 1\n",
    "k = 5\n",
    "\n",
    "x = torch.randn(bs,in_ch,l,l).to(device)\n",
    "weight = torch.randn(out_ch, in_ch, ker_dim, ker_dim).to(device) \n",
    "\n",
    "# print(x)\n",
    "# print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_trunc = trunc_conv(l,ker_dim,5)\n",
    "new_trunc = trunc_conv_new(l,ker_dim,5)\n",
    "\n",
    "new_trunc.weight = old_trunc.weight\n",
    "new_trunc.bias = old_trunc.bias\n",
    "\n",
    "new_trunc.to(device)\n",
    "old_trunc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_out = old_trunc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out = new_trunc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so far 4.8s and 1.3s\n",
    "old_out==new_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_net= Net('cnn_small', 512, x.shape, 2, 'clip')\n",
    "# r_net.to(device)\n",
    "\n",
    "# net = Net('cnn_small', 512, x.shape, 2, 'conv')\n",
    "# net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = x.clone().detach().cpu().numpy()\n",
    "\n",
    "r_vals = np.zeros_like(x_vals.reshape(-1,1,l**2)).squeeze()\n",
    "# print('r_vals: ',r_vals.shape)\n",
    "flat_kern = toeplitz_1_ch(weight[0,0].detach().cpu(), [l,l])\n",
    "# print('flat_kern:  ',flat_kern.shape)\n",
    "kern_mask = flat_kern != 0\n",
    "assert kern_mask.sum().sum() == (out_dim**2)*(ker_dim**2)\n",
    "r_scale = 1/kern_mask.sum(axis=0)\n",
    "# print('r_scale: ',r_scale.shape)\n",
    "\n",
    "for im in range(r_vals.shape[0]):\n",
    "    for i_kern in range(flat_kern.shape[0]):\n",
    "        kern_avg = np.dot(flat_kern[i_kern],x_vals[im,0].flatten())/(ker_dim**2)\n",
    "        # print('kern_avg: ',kern_avg.shape)\n",
    "        # print('kern_mask i_kern: ', kern_mask[i_kern])\n",
    "        temp = (flat_kern[i_kern]*x_vals[im,0].flatten()-kern_avg)*kern_mask[i_kern]\n",
    "        # print('temp: ',temp.shape)\n",
    "        # print('r_vals[im]: ', r_vals[im].shape)\n",
    "        r_vals[im] += temp \n",
    "        # break\n",
    "    # break\n",
    "    r_vals[im] /= r_scale\n",
    "# print(r_vals)\n",
    "\n",
    "r_vals = torch.tensor(r_vals).to(device)\n",
    "_, idx_top = torch.topk(r_vals,k) #(bs, out_dim, self.k)\n",
    "_, idx_bot = torch.topk(-1*r_vals,k)\n",
    "\n",
    "# print(idx_top)\n",
    "# # better to create mask instead of inplace \n",
    "z = torch.ones_like(r_vals)\n",
    "z[np.arange(r_vals.shape[0]),idx_top.T] = 0\n",
    "z[np.arange(r_vals.shape[0]),idx_bot.T] = 0\n",
    "\n",
    "# print(z)\n",
    "\n",
    "out = nn.functional.conv2d(z.view(x.shape)*x,weight.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = x.clone().detach().cpu().numpy()\n",
    "\n",
    "# r_vals is scales we want (bs, in_dim**2)\n",
    "r_vals = np.zeros_like(x_vals.reshape(-1,1,l**2)).squeeze()\n",
    "# print('r_vals: ',r_vals.shape)\n",
    "# flat kern is the conv in matrix form (out_dim**2, in_dim**2)\n",
    "flat_kern = toeplitz_1_ch(weight[0,0].detach().cpu(), [l,l])\n",
    "# print('flat_kern:  ',flat_kern.shape)\n",
    "kern_mask = flat_kern != 0\n",
    "# print('kern_mask: ', kern_mask.shape)\n",
    "assert kern_mask.sum().sum() == (out_dim**2)*(ker_dim**2)\n",
    "# every pixel gets same scale so r_scale is just (in_dim**2)\n",
    "r_scale = 1/kern_mask.sum(axis=0)\n",
    "# print('r_scale: ',r_scale.shape)\n",
    "\n",
    "# kern avg is just output of convolution \n",
    "kern_avg = nn.functional.conv2d(torch.tensor(x_vals),weight.detach().cpu()).numpy()\n",
    "kern_avg = kern_avg.reshape(x_vals.shape[0],out_dim**2)/(ker_dim**2)\n",
    "# print('kern_avg: ', kern_avg.shape)\n",
    "\n",
    "# temp shape is (bs, out_dim**2, in_dim**2)\n",
    "temp = x_vals.reshape(-1,1,l**2)*flat_kern\n",
    "temp -= kern_avg.reshape(-1,out_dim**2,1)\n",
    "temp *= kern_mask\n",
    "# print('temp: ', temp.shape)\n",
    "# sum over all kernels\n",
    "r_vals += temp.sum(axis=1)\n",
    "r_vals /= r_scale\n",
    "\n",
    "r_vals = torch.tensor(r_vals).to(device)\n",
    "_, idx_top = torch.topk(r_vals,k) #(bs, out_dim, self.k)\n",
    "_, idx_bot = torch.topk(-1*r_vals,k)\n",
    "\n",
    "# print(idx_top)\n",
    "# # better to create mask instead of inplace \n",
    "z = torch.ones_like(r_vals)\n",
    "z[np.arange(r_vals.shape[0]),idx_top.T] = 0\n",
    "z[np.arange(r_vals.shape[0]),idx_bot.T] = 0\n",
    "\n",
    "# print(z)\n",
    "\n",
    "out_new = nn.functional.conv2d(z.view(x.shape)*x,weight.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_vals = x.clone().detach().cpu().numpy()\n",
    "\n",
    "# r_vals = np.zeros_like(x_vals.reshape(-1,1,l**2)).squeeze()\n",
    "# print('r_vals: ',r_vals.shape)\n",
    "# flat_kern = toeplitz_1_ch(weight[0,0].detach().cpu(), [l,l])\n",
    "# print('flat_kern:  ',flat_kern.shape)\n",
    "# kern_mask = flat_kern != 0\n",
    "# assert kern_mask.sum().sum() == (out_dim**2)*(ker_dim**2)\n",
    "# r_scale = 1/kern_mask.sum(axis=0)\n",
    "# print('r_scale: ',r_scale.shape)\n",
    "\n",
    "# for im in range(r_vals.shape[0]):\n",
    "#     for i_kern in range(flat_kern.shape[0]):\n",
    "#         kern_avg = np.dot(flat_kern[i_kern],x_vals[im,0].flatten())/(ker_dim**2)\n",
    "#         print('kern_avg: ',kern_avg.shape, kern_avg)\n",
    "#         print('kern_mask: ', kern_mask.shape)\n",
    "#         print('kern_mask i_kern: ', kern_mask[i_kern].shape)\n",
    "#         temp = (flat_kern[i_kern]*x_vals[im,0].flatten())\n",
    "#         print('temp: ',temp.shape)\n",
    "#         temp -= kern_avg\n",
    "#         # print(temp)\n",
    "#         temp *= kern_mask[i_kern] \n",
    "#         print('temp: ',temp.shape)\n",
    "#         print('r_vals[im]: ', r_vals[im].shape)\n",
    "#         r_vals[im] += temp \n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular ind trunc\n",
    "\n",
    "print(x)\n",
    "x_vals = x.clone().detach() #(bs, in_features)\n",
    "print('x shape: ',x_vals.shape)\n",
    "temp = (x_vals.view(-1,1,in_ch)*weight).sum(axis=1) # (bs, out_features, in_features)\n",
    "print('sum of x@W along input pixel dimension: ',temp.shape)\n",
    "# x = torch.matmul(x,weight.T) #(bs, out_features)\n",
    "# print('x@W: ',x.shape)\n",
    "# print(temp.sum(axis=1))\n",
    "# sum over last axis gives you x@W\n",
    "# sum over first axis gives you largest contribution per in_feature\n",
    "# for x in bs, out_features element wise prod. of w and x\n",
    "_, idx_top = torch.topk(temp,k) #(bs, out_dim, self.k)\n",
    "_, idx_bot = torch.topk(-1*temp,k)\n",
    "print(idx_top)\n",
    "print(idx_bot)\n",
    "z = torch.ones(bs,in_ch).to(device)\n",
    "z[np.arange(bs),idx_top.T] = 0\n",
    "z[np.arange(bs),idx_bot.T] = 0\n",
    "print(z)\n",
    "# print(x)\n",
    "x = torch.matmul(z*x,weight.T)\n",
    "# print(z*x)\n",
    "print(x)\n",
    "\n",
    "# print(val_top.shape)\n",
    "\n",
    "# x -= val_top.sum(axis=-1)\n",
    "# x += val_bot.sum(axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.ones(bs,in_ch).to(device)\n",
    "print(z)\n",
    "print(idx_top)\n",
    "# print(z[np.arange(bs),idx_top.T])\n",
    "z[:,idx_top.T] = 0\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_top[np.arange]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w,h = 28,28\n",
    "kernel_dim = 3\n",
    "bs = 128\n",
    "in_ch = 3\n",
    "out_ch = 3\n",
    "\n",
    "image = torch.randn(bs,in_ch,w,h).to(device)\n",
    "kernel = torch.randn(out_ch,in_ch,kernel_dim,kernel_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_conv = torch.nn.Conv2d(in_ch,out_ch,kernel_dim,bias=True).to(device)\n",
    "\n",
    "conv = my_conv(in_ch,out_ch,kernel_dim,bias=True).to(device)\n",
    "conv.weight = nn_conv.weight\n",
    "conv.bias = nn_conv.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_out = nn_conv(image)\n",
    "out = conv(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn_out.shape)\n",
    "print(out.shape)\n",
    "print(((nn_out - out)<0.00001).sum() == np.prod(nn_out.shape))\n",
    "# print(((out - my_out)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('robust')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acfbcb7204aabebe0fded38f32a9455336bc712d2a72fb977b5ecc1782ddc648"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
