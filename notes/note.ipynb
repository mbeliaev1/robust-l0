{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# from utils.adv_trainer import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "root = os.path.abspath('/home/mbeliaev/home/code/robust-l0/')\n",
    "sys.path.append(root)\n",
    "\n",
    "from utils.models import *\n",
    "from utils.helpers import *\n",
    "from utils.attack import *\n",
    "\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 3\n",
    "\n",
    "mnist_1 = prep_data(root,256,'MNIST',beta=1)\n",
    "mnist_100 = prep_data(root,256,'MNIST',beta=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.) tensor(1.)\n",
      "tensor(0.4950) tensor(0.5050)\n"
     ]
    }
   ],
   "source": [
    "img = mnist_1['x_test'][0]\n",
    "print(img.min(),img.max())\n",
    "\n",
    "img = mnist_100['x_test'][0]\n",
    "print(img.min(),img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_ch = 1\n",
    "device = \"cuda:0\"\n",
    "b_norm_1 = nn.BatchNorm2d(in_ch).to(device)\n",
    "b_norm_100 = nn.BatchNorm2d(in_ch).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_1:  tensor(-0.4279, device='cuda:0', grad_fn=<MinBackward1>) tensor(2.7880, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "beta_100:  tensor(-0.3000, device='cuda:0', grad_fn=<MinBackward1>) tensor(1.9548, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "scaled_img_1 = b_norm_1(mnist_1['x_test'][20].to(device))\n",
    "print('beta_1: ',scaled_img_1.min(),scaled_img_1.max())\n",
    "\n",
    "scaled_img_100 = b_norm_100(mnist_100['x_test'][20].to(device))\n",
    "print('beta_100: ',scaled_img_100.min(),scaled_img_100.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0481e-08, device='cuda:0', grad_fn=<MeanBackward0>) tensor(1.8557e-06, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(1.0000, device='cuda:0', grad_fn=<StdBackward0>) tensor(0.7011, device='cuda:0', grad_fn=<StdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_img_1.mean(),scaled_img_100.mean())\n",
    "print(scaled_img_1.std(),scaled_img_100.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate accuracy and r acc\n",
    "def test_model(net_path, x, y, n_queries,  device):\n",
    "    # first load the x, y\n",
    "    input_shape = x[0].shape\n",
    "    config = json.load(open(net_path+'/setup.json','rb'))\n",
    "\n",
    "    net = Net(cfg_name=config['cfg_name'],\n",
    "        k = config['k'],\n",
    "        input_shape=input_shape,\n",
    "        trunc_type=config['trunc_type'])\n",
    "\n",
    "    net.to(device)\n",
    "    net.load_state_dict(torch.load(net_path+'/net.pth', map_location=device))\n",
    "    net.eval()\n",
    "\n",
    "    # next evaluate accuracy\n",
    "    acc = evaluate(net, x, y, device)\n",
    "    print(\"Clean accuracy of netwrk: %.2f\"%acc,\"%\")\n",
    "\n",
    "    # and robust accuracy\n",
    "    r_acc, _, _, _ = attack(net,config['perturb'],x,y,device=device,n_queries=n_queries,n_restarts=1,log_path='log.txt')\n",
    "    print(\"Robust accuracy of netwrk: %.2f\"%r_acc,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import matplotlib as mpl\n",
    "from matplotlib.gridspec import GridSpec\n",
    "# colors used\n",
    "ORANGE = '#FF9132'\n",
    "TEAL = '#0598B0'\n",
    "GREEN = '#008F00'\n",
    "PURPLE = '#8A2BE2'\n",
    "GRAY = '#969696'\n",
    "FIG_WIDTH = 9\n",
    "FIG_HEIGHT = 5\n",
    "FIG_NAME = 'analysis'\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Palatino\"],\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 10,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"lines.linewidth\": 2\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_mnist = np.array(mnist_simp.trunc[1].weight.data.detach().cpu())\n",
    "print(first_mnist.shape)\n",
    "\n",
    "first_cifar = np.array(cifar_simp.trunc[1].weight.data.detach().cpu())\n",
    "print(first_cifar.shape)\n",
    "\n",
    "n_rows = 1\n",
    "n_cols = 2\n",
    "n_bins = 10\n",
    "\n",
    "fig = plt.figure(figsize=(FIG_WIDTH,FIG_HEIGHT))\n",
    "gs = GridSpec(n_rows, n_cols, figure=fig)\n",
    "gs.update(wspace=0.2, hspace=0.5)\n",
    "# First lets make the 5 population charts\n",
    "mnist_axs = fig.add_subplot(gs[0,0])\n",
    "cifar_axs = fig.add_subplot(gs[0,1])\n",
    "fig.suptitle(\"Distribtion of Truncation Layer Weights\")\n",
    "\n",
    "# mnist first\n",
    "mnist_axs.hist(first_mnist,bins=n_bins,density=False)\n",
    "mnist_axs.set_title(\"MNIST\")\n",
    "cifar_axs.hist(first_cifar,bins=n_bins,density=False)\n",
    "cifar_axs.set_title(\"CIFAR\") \n",
    "\n",
    "\n",
    "mnist_axs.set_xlim(0,1)\n",
    "cifar_axs.set_xlim(0,1)\n",
    "# hist, edges = np.histogram(first_mnist, bins=n_bins, density=True)\n",
    "# print(edges)\n",
    "# print(edges[1:]-edges[0:-1])\n",
    "# print(np.arange(edges[0],edges[1],10))\n",
    "# axs = [fig.add_subplot(gs[i//n_cols,i%n_cols]) for i in range(n_cols*n_rows)]\n",
    "# for i in range(len(axs)):\n",
    "#     axs[i].set_title('expert %d'%i)\n",
    "#     hist, edges = np.histogram(expert_l2[i], bins=n_bins, density=True)\n",
    "#     # print(edges[1:]-edges[0:-1])\n",
    "#     # print(np.arange(edges[0],edges[1],10))\n",
    "#     axs[i].bar(x=edges[0:-1],height=hist,align='edge',width=edges[1:]-edges[0:-1])\n",
    "#     axs[i].set_xticks(edges[0:100:10])\n",
    "#     axs[i].set_xticklabels(np.round(edges[0:100:10],decimals=1))\n",
    "#     # let us also plot the MLE of the Gamma Dist\n",
    "#     temp_s = np.log(np.mean(expert_l2[i])) - np.mean(np.log(expert_l2[i]))\n",
    "#     alphas[i] = (3-temp_s+np.sqrt((3-temp_s**2)+24*temp_s))/(12*temp_s)\n",
    "#     scales[i] = np.mean(expert_l2[i])/alphas[i]\n",
    "#     pdf = stats.gamma.pdf(x=edges, a=alphas[i], scale = scales[i])\n",
    "#     axs[i].plot(edges, pdf, c='r', label=r'$\\alpha=%.2f, \\beta=%.2f$'%(alphas[i],1/scales[i]))\n",
    "#     # mean = np.mean(expert_l2[i])\n",
    "#     # std = np.std(expert_l2[i])\n",
    "#     # # axs[i].axvline(x=mean,c='r',label='mean')\n",
    "#     # # axs[i].axvline(x=std,c='g',label='std')\n",
    "#     axs[i].set_xlim(0,max(expert_l2[i]))\n",
    "#     axs[i].legend()\n",
    "# axs[0].legend()\n",
    "\n",
    "    # print(hist)\n",
    "# for i in range(len(ax_bars)):\n",
    "#     ax_bars[i].set_xlim(-0.5,9.5)\n",
    "#     ax_bars[i].set_ylim(0,1)\n",
    "#     ax_bars[i].set_xticks([])\n",
    "#     ax_bars[i].set_yticks([])\n",
    "#     ax_bars[i].bar(np.arange(10),all_noise_levels[i])\n",
    "\n",
    "\n",
    "# ax_bars[6].set_xticks([0,1,2,3,4,5,6,7,8,9])\n",
    "# ax_bars[6].set_xticklabels([1,2,3,4,5,6,7,8,9,10])\n",
    "# ax_bars[6].set_xlabel('Annotator')\n",
    "\n",
    "# ax_bars[3].set_yticks([0,0.5,1])\n",
    "# ax_bars[3].set_yticklabels([0.0,0.5,1.0])\n",
    "# ax_bars[3].set_ylabel('Expertise Level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_mnist = np.array(mnist_simp.trunc[1].weight.data.detach().cpu())\n",
    "print(first_mnist.shape)\n",
    "\n",
    "first_cifar = np.array(cifar_simp.trunc[1].weight.data.detach().cpu())\n",
    "print(first_cifar.shape)\n",
    "\n",
    "n_rows = 1\n",
    "n_cols = 2\n",
    "\n",
    "fig = plt.figure(figsize=(FIG_WIDTH,FIG_HEIGHT))\n",
    "gs = GridSpec(n_rows, n_cols, figure=fig)\n",
    "gs.update(wspace=0.2, hspace=0.5)\n",
    "# First lets make the 5 population charts\n",
    "mnist_axs = fig.add_subplot(gs[0,0])\n",
    "cifar_axs = fig.add_subplot(gs[0,1])\n",
    "fig.suptitle(\"Value of Truncation Layer Weights\")\n",
    "\n",
    "# mnist first\n",
    "img_mnist = mnist_axs.imshow(np.reshape(first_mnist,(28,28)),cmap='Greens')\n",
    "img_mnist.set_clim(0,1)\n",
    "mnist_axs.set_title(\"MNIST\")\n",
    "cbar = mnist_axs.figure.colorbar(img_mnist, ax=mnist_axs)\n",
    "# cifar next\n",
    "img_cifar = cifar_axs.imshow(np.reshape(first_cifar[0:32*32],(32,32)),cmap='Blues')\n",
    "img_cifar.set_clim(0,1)\n",
    "cifar_axs.set_title(\"CIFAR (1st channel)\")\n",
    "cbar = cifar_axs.figure.colorbar(img_cifar, ax=cifar_axs)\n",
    "\n",
    "    # fig, ax = plt.subplots()\n",
    "    # im = ax.imshow(prosocial_percentages, cmap='Greens')\n",
    "    # im.set_clim(0, 1)\n",
    "    # ax.set_xticks(np.arange(db))\n",
    "    # ax.set_yticks(np.arange(db))\n",
    "    # # ... and label them with the respective list entries\n",
    "    # ax.set_xticklabels(b0)\n",
    "    # ax.set_yticklabels(sorted(b1, reverse=True)) # Reverse the y labels\n",
    "    # ax.set_xlabel('$x_1$')\n",
    "    # ax.set_ylabel('$y_1$')\n",
    "    # cbar = ax.figure.colorbar(im, ax=ax)\n",
    "    # cbar.ax.set_ylabel('Percentage of Prosocial Runs', rotation=-90, va=\"bottom\")\n",
    "# mnist_axs.set_xlim(0,1)\n",
    "# cifar_axs.set_xlim(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at the distribution of the actual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_data(i,data):\n",
    "    pixels = []\n",
    "    for batch in data:\n",
    "        for img in batch:\n",
    "            pixels.append(img.flatten()[i].item())\n",
    "    pixels = np.asarray(pixels)\n",
    "    return pixels.mean(), pixels.std()\n",
    "\n",
    "def get_pixel_raw(i,data):\n",
    "    pixels = []\n",
    "    for batch in data:\n",
    "        for img in batch:\n",
    "            pixels.append(img.flatten()[i].item())\n",
    "    pixels = np.asarray(pixels)\n",
    "    return pixels\n",
    "\n",
    "def get_dataset_raw(data):\n",
    "    num_pixels = data[0][0].numel()\n",
    "    out = get_pixel_raw(0, data)\n",
    "    for px in range(1, num_pixels):\n",
    "        out = np.concatenate((out,get_pixel_raw(px, data)))\n",
    "\n",
    "    return out\n",
    "\n",
    "def get_dataset_stats(data):\n",
    "    ch = data[0][0].shape[0]\n",
    "    l = data[0][0].shape[1]\n",
    "    w = data[0][0].shape[2]\n",
    "    means = np.zeros((ch,l,w))\n",
    "    stds = np.zeros((ch,l,w))\n",
    "\n",
    "    for i_ch in range(ch):\n",
    "        for i_l in range(l):\n",
    "            for i_w in range(w):\n",
    "                i_pixel = i_ch*(l*w) + i_l*w + i_w\n",
    "                means[i_ch,i_l,i_w], stds[i_ch,i_l,i_w] = get_pixel_data(i_pixel, data)\n",
    "\n",
    "    \n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_mnist = np.array(mnist_simp.trunc[1].weight.data.detach().cpu())\n",
    "print(first_mnist.shape)\n",
    "\n",
    "first_cifar = np.array(cifar_simp.trunc[1].weight.data.detach().cpu())\n",
    "print(first_cifar.shape)\n",
    "\n",
    "n_rows = 1\n",
    "n_cols = 2\n",
    "\n",
    "fig = plt.figure(figsize=(FIG_WIDTH,FIG_HEIGHT))\n",
    "gs = GridSpec(n_rows, n_cols, figure=fig)\n",
    "gs.update(wspace=0.2, hspace=0.5)\n",
    "# First lets make the 5 population charts\n",
    "mnist_axs = fig.add_subplot(gs[0,0])\n",
    "cifar_axs = fig.add_subplot(gs[0,1])\n",
    "fig.suptitle(\"Average value of pixels in dataset\")\n",
    "\n",
    "# mnist first\n",
    "means, _ = get_dataset_stats(mnist['x_test'])\n",
    "img_mnist = mnist_axs.imshow(means[0],cmap='Greens')\n",
    "img_mnist.set_clim(0,1)\n",
    "mnist_axs.set_title(\"MNIST\")\n",
    "cbar = mnist_axs.figure.colorbar(img_mnist, ax=mnist_axs)\n",
    "# cifar next\n",
    "means, _ = get_dataset_stats(cifar['x_test'])\n",
    "img_cifar = cifar_axs.imshow(means[0],cmap='Blues')\n",
    "img_cifar.set_clim(0,1)\n",
    "cifar_axs.set_title(\"CIFAR (1st channel)\")\n",
    "cbar = cifar_axs.figure.colorbar(img_cifar, ax=cifar_axs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_mnist = np.array(mnist_simp.trunc[1].weight.data.detach().cpu())\n",
    "print(first_mnist.shape)\n",
    "\n",
    "first_cifar = np.array(cifar_simp.trunc[1].weight.data.detach().cpu())\n",
    "print(first_cifar.shape)\n",
    "\n",
    "n_rows = 1\n",
    "n_cols = 2\n",
    "n_bins = 10\n",
    "\n",
    "fig = plt.figure(figsize=(FIG_WIDTH,FIG_HEIGHT))\n",
    "gs = GridSpec(n_rows, n_cols, figure=fig)\n",
    "gs.update(wspace=0.2, hspace=0.5)\n",
    "# First lets make the 5 population charts\n",
    "mnist_axs = fig.add_subplot(gs[0,0])\n",
    "cifar_axs = fig.add_subplot(gs[0,1])\n",
    "fig.suptitle(\"Distribtion Pixel Values\")\n",
    "\n",
    "# mnist first\n",
    "data = get_dataset_raw(mnist['x_test'][0:20])\n",
    "mnist_axs.hist(data,bins=n_bins,density=False)\n",
    "mnist_axs.set_title(\"MNIST\")\n",
    "\n",
    "# cifar next\n",
    "data = get_dataset_raw(cifar['x_test'][0:20])\n",
    "cifar_axs.hist(data,bins=n_bins,density=False)\n",
    "cifar_axs.set_title(\"CIFAR\") \n",
    "\n",
    "\n",
    "mnist_axs.set_xlim(0,1)\n",
    "cifar_axs.set_xlim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_dataset_raw(mnist['x_test'][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%.2f\"%(100*(data==0).sum()/len(data)),\"%\",\"of data is zero\")\n",
    "print(\"%.2f\"%(100*(data==1).sum()/len(data)),\"%\",\"of data is one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many ones are there per image? \n",
    "num_ones = []\n",
    "for batch in mnist['x_test']:\n",
    "    for img in batch:\n",
    "        num_ones.append((img==1).sum().item())\n",
    "\n",
    "num_zeros = []\n",
    "for batch in mnist['x_test']:\n",
    "    for img in batch:\n",
    "        num_zeros.append((img==0).sum().item())\n",
    "\n",
    "print('On average %.2f'%(np.asarray(num_ones).mean()/(.28*28)),\"%\",\"of pixels are one\",np.asarray(num_ones).mean())\n",
    "print('On average %.2f'%(np.asarray(num_zeros).mean()/(.28*28)),\"%\",\"of pixels are zero\",np.asarray(num_zeros).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many ones are there per image? \n",
    "num_ones = []\n",
    "for batch in cifar['x_test']:\n",
    "    for img in batch:\n",
    "        num_ones.append((img==1).sum().item())\n",
    "\n",
    "num_zeros = []\n",
    "for batch in cifar['x_test']:\n",
    "    for img in batch:\n",
    "        num_zeros.append((img==0).sum().item())\n",
    "\n",
    "print('On average %.2f'%(np.asarray(num_ones).mean()/(.32*32*3)),\"%\",\"of pixels are one\",np.asarray(num_ones).mean())\n",
    "print('On average %.2f'%(np.asarray(num_zeros).mean()/(.32*32*3)),\"%\",\"of pixels are zero\",np.asarray(num_zeros).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('robust')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acfbcb7204aabebe0fded38f32a9455336bc712d2a72fb977b5ecc1782ddc648"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
